import json
import time
from typing import List, Dict, Any
from improved_rag import ImprovedRAG
from tabulate import tabulate
import matplotlib.pyplot as plt
import seaborn as sns


class RAGEvaluator:
    def __init__(self):
        self.rag = ImprovedRAG(use_openai=False)
        self.test_results = []
        
    def setup_test_data(self):
        """è¨­ç½®æ¸¬è©¦æ•¸æ“š"""
        texts = [
            "LangChain æ˜¯ä¸€å€‹å¼·å¤§çš„æ¡†æ¶ï¼Œç”¨ä¾†å»ºæ§‹ LLM æ‡‰ç”¨ã€‚",
            "FAISS æ˜¯ç”± Facebook AI æä¾›çš„å‘é‡æª¢ç´¢è³‡æ–™åº«ã€‚",
            "ä½ å¯ä»¥å°‡æ–‡ä»¶è½‰æ›æˆ Embeddingsï¼Œç„¶å¾Œç”¨ FAISS åšç›¸ä¼¼åº¦æœå°‹ã€‚",
            "Kevin Sinæ˜¯NSGçš„å¤§PMã€‚",
            "Danny Huangæ˜¯NSGçš„ç¸½ç¶“ç†ã€‚",
            "NSGæ˜¯ä¸€å€‹æŠ€è¡“åœ˜éšŠï¼Œè² è²¬é–‹ç™¼AIç›¸é—œç”¢å“ã€‚",
            "RAGç³»çµ±çµåˆäº†æª¢ç´¢å’Œç”ŸæˆæŠ€è¡“ï¼Œæä¾›æ›´æº–ç¢ºçš„ç­”æ¡ˆã€‚",
            "Pythonæ˜¯ä¸€ç¨®æµè¡Œçš„ç¨‹å¼èªè¨€ï¼Œå»£æ³›ç”¨æ–¼AIé–‹ç™¼ã€‚",
            "æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ã€‚",
            "è‡ªç„¶èªè¨€è™•ç†å¹«åŠ©é›»è…¦ç†è§£äººé¡èªè¨€ã€‚",
            "å‘é‡è³‡æ–™åº«å°ˆé–€ç”¨æ–¼å„²å­˜å’Œæª¢ç´¢é«˜ç¶­å‘é‡æ•¸æ“šã€‚",
            "Embeddingæ¨¡å‹å°‡æ–‡æœ¬è½‰æ›ç‚ºæ•¸å€¼å‘é‡è¡¨ç¤ºã€‚"
        ]
        
        self.rag.setup_documents(texts)
        
    def create_test_cases(self) -> List[Dict[str, Any]]:
        """å‰µå»ºæ¸¬è©¦æ¡ˆä¾‹"""
        return [
            {
                "query": "Kevin Sinæ˜¯èª°ï¼Ÿ",
                "expected_keywords": ["Kevin Sin", "PM", "NSG"],
                "category": "äººç‰©æŸ¥è©¢"
            },
            {
                "query": "Danny Huangçš„è·ä½ï¼Ÿ",
                "expected_keywords": ["Danny Huang", "ç¸½ç¶“ç†", "NSG"],
                "category": "è·ä½æŸ¥è©¢"
            },
            {
                "query": "èª°æ˜¯NSGçš„PMï¼Ÿ",
                "expected_keywords": ["Kevin Sin", "PM"],
                "category": "åå‘æŸ¥è©¢"
            },
            {
                "query": "ä»€éº¼æ˜¯LangChainï¼Ÿ",
                "expected_keywords": ["LangChain", "æ¡†æ¶", "LLM"],
                "category": "æŠ€è¡“æŸ¥è©¢"
            },
            {
                "query": "FAISSæ˜¯ä»€éº¼ï¼Ÿ",
                "expected_keywords": ["FAISS", "å‘é‡", "æª¢ç´¢", "è³‡æ–™åº«"],
                "category": "æŠ€è¡“æŸ¥è©¢"
            },
            {
                "query": "å¦‚ä½•ä½¿ç”¨å‘é‡æœå°‹ï¼Ÿ",
                "expected_keywords": ["Embeddings", "FAISS", "ç›¸ä¼¼åº¦"],
                "category": "æŠ€è¡“æŒ‡å°"
            },
            {
                "query": "NSGåœ˜éšŠåšä»€éº¼ï¼Ÿ",
                "expected_keywords": ["NSG", "æŠ€è¡“åœ˜éšŠ", "AI", "ç”¢å“"],
                "category": "çµ„ç¹”æŸ¥è©¢"
            },
            {
                "query": "æ©Ÿå™¨å­¸ç¿’æ˜¯ä»€éº¼ï¼Ÿ",
                "expected_keywords": ["æ©Ÿå™¨å­¸ç¿’", "äººå·¥æ™ºæ…§", "åˆ†æ”¯"],
                "category": "æ¦‚å¿µæŸ¥è©¢"
            }
        ]
    
    def evaluate_retrieval_quality(self, query: str, retrieved_docs: List, expected_keywords: List[str]) -> Dict[str, float]:
        """è©•ä¼°æª¢ç´¢è³ªé‡"""
        if not retrieved_docs:
            return {"precision": 0.0, "recall": 0.0, "f1": 0.0}
        
        # å°‡æ‰€æœ‰æª¢ç´¢åˆ°çš„æ–‡æœ¬åˆä½µ
        retrieved_text = " ".join([doc.page_content for doc in retrieved_docs]).lower()
        
        # è¨ˆç®—é—œéµè©åŒ¹é…
        matched_keywords = []
        for keyword in expected_keywords:
            if keyword.lower() in retrieved_text:
                matched_keywords.append(keyword)
        
        precision = len(matched_keywords) / len(expected_keywords) if expected_keywords else 0
        recall = len(matched_keywords) / len(expected_keywords) if expected_keywords else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "matched_keywords": matched_keywords,
            "total_keywords": len(expected_keywords)
        }
    
    def evaluate_answer_quality(self, answer: str, expected_keywords: List[str]) -> Dict[str, float]:
        """è©•ä¼°ç­”æ¡ˆè³ªé‡"""
        answer_lower = answer.lower()
        
        matched_keywords = []
        for keyword in expected_keywords:
            if keyword.lower() in answer_lower:
                matched_keywords.append(keyword)
        
        keyword_coverage = len(matched_keywords) / len(expected_keywords) if expected_keywords else 0
        
        # ç­”æ¡ˆé•·åº¦è©•åˆ† (é©ä¸­çš„é•·åº¦å¾—åˆ†æ›´é«˜)
        length_score = 1.0
        if len(answer) < 10:
            length_score = 0.3  # å¤ªçŸ­
        elif len(answer) > 200:
            length_score = 0.7  # å¤ªé•·
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å« "Will:" å‰ç¶´
        format_score = 1.0 if answer.startswith("Will:") else 0.5
        
        return {
            "keyword_coverage": keyword_coverage,
            "length_score": length_score,
            "format_score": format_score,
            "matched_keywords": matched_keywords
        }
    
    def run_single_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """é‹è¡Œå–®å€‹æ¸¬è©¦æ¡ˆä¾‹"""
        query = test_case["query"]
        expected_keywords = test_case["expected_keywords"]
        category = test_case["category"]
        
        start_time = time.time()
        
        # åŸ·è¡ŒRAGæŸ¥è©¢
        result = self.rag.answer_question(query)
        
        end_time = time.time()
        response_time = end_time - start_time
        
        # è©•ä¼°æª¢ç´¢è³ªé‡
        retrieval_metrics = self.evaluate_retrieval_quality(
            query, result["source_documents"], expected_keywords
        )
        
        # è©•ä¼°ç­”æ¡ˆè³ªé‡
        answer_metrics = self.evaluate_answer_quality(
            result["answer"], expected_keywords
        )
        
        return {
            "query": query,
            "category": category,
            "answer": result["answer"],
            "response_time": response_time,
            "retrieval_metrics": retrieval_metrics,
            "answer_metrics": answer_metrics,
            "source_count": len(result["source_documents"]),
            "expected_keywords": expected_keywords
        }
    
    def run_evaluation(self) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´è©•ä¼°"""
        print("ğŸš€ é–‹å§‹RAGç³»çµ±è©•ä¼°...")
        
        # è¨­ç½®æ¸¬è©¦æ•¸æ“š
        self.setup_test_data()
        
        # ç²å–æ¸¬è©¦æ¡ˆä¾‹
        test_cases = self.create_test_cases()
        
        # é‹è¡Œæ‰€æœ‰æ¸¬è©¦
        self.test_results = []
        for i, test_case in enumerate(test_cases, 1):
            print(f"ğŸ“ é‹è¡Œæ¸¬è©¦ {i}/{len(test_cases)}: {test_case['query']}")
            result = self.run_single_test(test_case)
            self.test_results.append(result)
        
        # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
        overall_stats = self.calculate_overall_stats()
        
        return {
            "test_results": self.test_results,
            "overall_stats": overall_stats
        }
    
    def calculate_overall_stats(self) -> Dict[str, float]:
        """è¨ˆç®—ç¸½é«”çµ±è¨ˆæ•¸æ“š"""
        if not self.test_results:
            return {}
        
        # æª¢ç´¢æŒ‡æ¨™
        avg_precision = sum(r["retrieval_metrics"]["precision"] for r in self.test_results) / len(self.test_results)
        avg_recall = sum(r["retrieval_metrics"]["recall"] for r in self.test_results) / len(self.test_results)
        avg_f1 = sum(r["retrieval_metrics"]["f1"] for r in self.test_results) / len(self.test_results)
        
        # ç­”æ¡ˆæŒ‡æ¨™
        avg_keyword_coverage = sum(r["answer_metrics"]["keyword_coverage"] for r in self.test_results) / len(self.test_results)
        avg_format_score = sum(r["answer_metrics"]["format_score"] for r in self.test_results) / len(self.test_results)
        
        # æ€§èƒ½æŒ‡æ¨™
        avg_response_time = sum(r["response_time"] for r in self.test_results) / len(self.test_results)
        avg_source_count = sum(r["source_count"] for r in self.test_results) / len(self.test_results)
        
        return {
            "avg_precision": avg_precision,
            "avg_recall": avg_recall,
            "avg_f1": avg_f1,
            "avg_keyword_coverage": avg_keyword_coverage,
            "avg_format_score": avg_format_score,
            "avg_response_time": avg_response_time,
            "avg_source_count": avg_source_count,
            "total_tests": len(self.test_results)
        }
    
    def print_detailed_results(self):
        """æ‰“å°è©³ç´°çµæœ"""
        print("\n" + "="*80)
        print("ğŸ“Š RAGç³»çµ±è©•ä¼°è©³ç´°çµæœ")
        print("="*80)
        
        for i, result in enumerate(self.test_results, 1):
            print(f"\nğŸ” æ¸¬è©¦ {i}: {result['category']}")
            print(f"å•é¡Œ: {result['query']}")
            print(f"å›ç­”: {result['answer']}")
            print(f"éŸ¿æ‡‰æ™‚é–“: {result['response_time']:.2f}ç§’")
            
            # æª¢ç´¢æŒ‡æ¨™
            rm = result['retrieval_metrics']
            print(f"æª¢ç´¢æŒ‡æ¨™ - Precision: {rm['precision']:.2f}, Recall: {rm['recall']:.2f}, F1: {rm['f1']:.2f}")
            
            # ç­”æ¡ˆæŒ‡æ¨™
            am = result['answer_metrics']
            print(f"ç­”æ¡ˆæŒ‡æ¨™ - é—œéµè©è¦†è“‹ç‡: {am['keyword_coverage']:.2f}, æ ¼å¼åˆ†æ•¸: {am['format_score']:.2f}")
            
            print(f"åŒ¹é…é—œéµè©: {am['matched_keywords']}")
            print("-" * 50)
    
    def print_summary(self, overall_stats: Dict[str, float]):
        """æ‰“å°ç¸½çµ"""
        print("\n" + "="*80)
        print("ğŸ“ˆ RAGç³»çµ±è©•ä¼°ç¸½çµ")
        print("="*80)
        
        summary_data = [
            ["ç¸½æ¸¬è©¦æ•¸é‡", overall_stats['total_tests']],
            ["å¹³å‡ç²¾ç¢ºç‡", f"{overall_stats['avg_precision']:.2f}"],
            ["å¹³å‡å¬å›ç‡", f"{overall_stats['avg_recall']:.2f}"],
            ["å¹³å‡F1åˆ†æ•¸", f"{overall_stats['avg_f1']:.2f}"],
            ["å¹³å‡é—œéµè©è¦†è“‹ç‡", f"{overall_stats['avg_keyword_coverage']:.2f}"],
            ["å¹³å‡æ ¼å¼åˆ†æ•¸", f"{overall_stats['avg_format_score']:.2f}"],
            ["å¹³å‡éŸ¿æ‡‰æ™‚é–“", f"{overall_stats['avg_response_time']:.2f}ç§’"],
            ["å¹³å‡ä¾†æºæ–‡æª”æ•¸", f"{overall_stats['avg_source_count']:.1f}"]
        ]
        
        print(tabulate(summary_data, headers=["æŒ‡æ¨™", "å€¼"], tablefmt="grid"))
        
        # æ€§èƒ½è©•ç´š
        overall_score = (overall_stats['avg_f1'] + overall_stats['avg_keyword_coverage']) / 2
        if overall_score >= 0.8:
            grade = "å„ªç§€ ğŸŒŸ"
        elif overall_score >= 0.6:
            grade = "è‰¯å¥½ ğŸ‘"
        elif overall_score >= 0.4:
            grade = "æ™®é€š ğŸ‘Œ"
        else:
            grade = "éœ€æ”¹é€² âš ï¸"
        
        print(f"\nğŸ¯ æ•´é«”è©•ç´š: {grade} (åˆ†æ•¸: {overall_score:.2f})")
    
    def save_results(self, filename="rag_evaluation_results.json"):
        """ä¿å­˜è©•ä¼°çµæœ"""
        evaluation_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "test_results": self.test_results,
            "overall_stats": self.calculate_overall_stats()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(evaluation_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ è©•ä¼°çµæœå·²ä¿å­˜åˆ°: {filename}")


def main():
    """ä¸»å‡½æ•¸"""
    evaluator = RAGEvaluator()
    
    # é‹è¡Œè©•ä¼°
    results = evaluator.run_evaluation()
    
    # é¡¯ç¤ºçµæœ
    evaluator.print_detailed_results()
    evaluator.print_summary(results["overall_stats"])
    
    # ä¿å­˜çµæœ
    evaluator.save_results()
    
    print("\nâœ… è©•ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()
