import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.docstore.document import Document
import re
import regex
from dotenv import load_dotenv
load_dotenv()


def generate_variants(texts: list) -> list:
    variants = set()
    PUNCT = r"ã€‚ï¼›ï¼›,.;!?ï¼ï¼Ÿã€"
    def clean_sentence(text: str) -> str:
        return re.sub(fr"^[\s{PUNCT}]+|[\s{PUNCT}]+$", "", text)

    for text in texts:
        # å»æ‰å‰å¾Œä¸­è‹±æ–‡æ¨™é»
        sentence = clean_sentence(text)

        if not sentence:
            continue

        # åŸå¥
        variants.add(sentence)

        # å¦‚æœåŒ…å«ã€Œæ˜¯ã€ï¼Œå˜—è©¦ç”Ÿæˆè®Šé«”
        if "æ˜¯" in sentence:
            parts = sentence.split("æ˜¯", 1)
            if len(parts) == 2:
                left, right = parts[0].strip(), parts[1].strip()

                if left and right:
                    # å€’è£å¥
                    variants.add(f"{right}æ˜¯{left}ã€‚")

                    # å•å¥ï¼ˆèª°æ˜¯ rightï¼‰
                    variants.add(f"èª°æ˜¯{right}ï¼Ÿæ˜¯{left}ã€‚")

                    # å•å¥ï¼ˆleft æ˜¯èª°ï¼‰
                    variants.add(f"{left}æ˜¯èª°ï¼Ÿæ˜¯{right}ã€‚")

                    # å•å¥ï¼ˆright æ˜¯ä»€éº¼ï¼‰
                    variants.add(f"{right}æ˜¯ä»€éº¼ï¼Ÿæ˜¯{left}ã€‚")

    return list(variants)


# ç’°å¢ƒè®Šæ•¸
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
print("OPENAI_API_KEY:", OPENAI_API_KEY)

# 1. æº–å‚™æ–‡æœ¬è³‡æ–™
texts = [
    "LangChain æ˜¯ä¸€å€‹å¼·å¤§çš„æ¡†æ¶ï¼Œç”¨ä¾†å»ºæ§‹ LLM æ‡‰ç”¨ã€‚",
    "FAISS æ˜¯ç”± Facebook AI æä¾›çš„å‘é‡æª¢ç´¢è³‡æ–™åº«ã€‚",
    "ä½ å¯ä»¥å°‡æ–‡ä»¶è½‰æ›æˆ Embeddingsï¼Œç„¶å¾Œç”¨ FAISS åšç›¸ä¼¼åº¦æœå°‹ã€‚",
    "Kevin Sinæ˜¯NSGçš„å¤§PMã€‚",
    "Danny Huangæ˜¯NSGçš„ç¸½ç¶“ç†ã€‚",
]

texts = generate_variants(texts)


docs = [
    Document(page_content=t, metadata={"index": i})
    for i, t in enumerate(texts)
]

# 2. å»ºç«‹ Embeddings èˆ‡ FAISS å‘é‡è³‡æ–™åº«
# embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
"""
ä½† llama3.2 æ˜¯èŠå¤©/ç”Ÿæˆæ¨¡å‹ï¼Œä¸æ˜¯å‘é‡åµŒå…¥æ¨¡å‹ã€‚ç”¨å®ƒåš embedding 
æœƒå¾ˆå·®ï¼ˆæˆ–ç›´æ¥ä¸å°ï¼‰ï¼ŒFAISS ç•¶ç„¶å°±æŠ“ä¸åˆ°ã€ŒKevin Sinã€é‚£å¥ã€‚ğŸ‘‡çµ¦ä½ å…©å€‹å¯ç”¨æ–¹æ¡ˆèˆ‡æœ€å°ä¿®æ­£ã€‚
"""
# embeddings = OllamaEmbeddings(model="llama3.2:latest")
# embeddings = OpenAIEmbeddings(model="text-embedding-3-small")  # æˆ– text-embedding-3-large
# ollama pull nomic-embed-text
from langchain_community.embeddings import OllamaEmbeddings
embeddings = OllamaEmbeddings(model="nomic-embed-text")  # æˆ– "mxbai-embed-large" 


"""
1) ç”¨ã€Œé¤µ Cosineã€çš„æ–¹å¼å»ºç´¢å¼•

FAISS åœ¨ LangChain é è¨­æ˜¯ L2 è·é›¢ï¼›è€Œå¥å‘é‡å¸¸ç”¨ Cosine ç›¸ä¼¼åº¦ã€‚åšæ³•æ˜¯æŠŠå‘é‡ å…ˆåš L2 æ­£è¦åŒ– å†ç”¨ L2 æœå°‹ï¼ˆç­‰æ•ˆæ–¼ Cosineï¼‰ã€‚
ğŸ‘‰ åªè¦åœ¨ from_documents åŠ  normalize_L2=Trueï¼š
"""
db = FAISS.from_documents(docs, embeddings, normalize_L2=True)

# 3. å»ºç«‹æª¢ç´¢å™¨
"""
2) k èª¿å¤§ã€MMR æ›´ç©©
"""
# retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 4})
retriever = db.as_retriever(
    search_type="mmr",                # æ”¹æˆ mmr
    search_kwargs={"k": 8, "fetch_k": 20, "lambda_mult": 0.5}
)
print("=== DEBUG Retrieved ===")
for r in retriever.get_relevant_documents("èª°æ˜¯ Kevin Sin"):
    print(r.metadata, r.page_content)
print("=======================")

# 4. çµåˆ LLM å•ç­”
# llm = ChatOpenAI(
#     model="gpt-4o-mini",  # ä¹Ÿå¯ä»¥ç”¨ "gpt-4o" æˆ– "gpt-3.5-turbo"
#     temperature=0
# )

llm = ChatOllama(model="llama3.2:latest")

system_prompt = """
å›ç­”å¿…é ˆä»¥ã€ŒWill: ã€é–‹é ­ï¼Œä¸”æœ€å¤šä¸‰å¥è©±ã€‚
å¦‚æœ context ä¸­æ‰¾åˆ°æŸäººå°æ‡‰çš„èº«ä»½æˆ–è·ç¨±ï¼Œå°±ç›´æ¥è¼¸å‡ºè©²èº«ä»½æˆ–è·ç¨±ï¼Œå¦‚æœåœ¨è³‡æ–™åº«ç™¼ç¾å°±ä¸è¦ç”¨è‡ªå·±çš„çŸ¥è­˜è£œå……ã€‚
å¦‚æœ context ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œå°±å›ç­”ã€Œæˆ‘ä¸çŸ¥é“ã€ã€‚
å¦‚æœæª¢ç´¢çµæœé¡¯ç¤ºæŸäººèº«ä»½æˆ–è·ç¨±ï¼Œå°±ç›´æ¥å›ç­”è©²èº«ä»½ã€‚

{context}
"""

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

# å»ºç«‹ QA chain
question_answer_chain = create_stuff_documents_chain(llm, prompt)
qa_chain = create_retrieval_chain(retriever, question_answer_chain)

# 5. å•å•é¡Œ
query = input("è«‹è¼¸å…¥ä½ çš„å•é¡Œ: ")
result = qa_chain.invoke({"input": query})
db.save_local("faiss_index")
print("å•é¡Œ:", query)
print("å›ç­”:", result["answer"])


